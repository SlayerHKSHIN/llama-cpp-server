import os
import time
import uuid
from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.responses import JSONResponse
from fastapi.security.api_key import APIKeyHeader
from pydantic import BaseModel, Field
from llama_cpp import Llama
import threading
from typing import List, Optional, Union
import traceback
import json

# --- API Key ì„¤ì • ---
VALID_API_KEYS = {
    os.getenv("API_KEY_1", ""),
    os.getenv("API_KEY_2", ""),
    os.getenv("API_KEY_3", ""),
} - {""}
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)

def get_api_key(key: str = Depends(api_key_header)):
    if key not in VALID_API_KEYS:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid API Key")
    return key

# --- FastAPI ì•± ---
app = FastAPI(title="llama-cpp-python Chat API")

# --- ë©”ì‹œì§€ êµ¬ì¡° ---
class Message(BaseModel):
    role: str
    content: str

class ToolFunction(BaseModel):
    name: str
    description: Optional[str] = None
    parameters: dict

class Tool(BaseModel):
    type: str
    function: ToolFunction

class ChatRequest(BaseModel):
    messages: List[Message]
    max_tokens: int = 128
    temperature: Optional[float] = 0.7
    tools: Optional[List[Tool]] = None
    tool_choice: Optional[Union[str, dict]] = None
    stream: Optional[bool] = False
    logprobs: Optional[bool] = False
    conversation_rounds: int = Field(default=100)
    system_prompt: Optional[str] = None
    user_prompt_input: Optional[str] = None
    rag_file_path: Optional[str] = None   # <-- RAG íŒŒì¼ ê²½ë¡œ ì¶”ê°€!
    rag_query: Optional[str] = None       # <-- RAGìš© ì¿¼ë¦¬ ì¶”ê°€!

class ChoiceMessage(BaseModel):
    role: str
    content: str

class ChatChoice(BaseModel):
    index: int
    message: ChoiceMessage
    finish_reason: str = "stop"

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatChoice]
    usage: Usage

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
MODEL_PATH = os.environ["MODEL_PATH"]
THREADS    = int(os.getenv("THREADS", "0"))
CTX_SIZE   = int(os.getenv("CTX_SIZE", "4096"))
GPU_LAYERS = int(os.getenv("GPU_LAYERS", "40"))
LOW_VRAM   = bool(int(os.getenv("LOW_VRAM", "0")))
MAX_CONCURRENT = int(os.getenv("MAX_CONCURRENT_REQUESTS", "1"))
NUM_MODELS = int(os.getenv("MAX_CONCURRENT_REQUESTS", "1"))  # ì‚¬ìš©í•  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜

# --- ê°„ë‹¨ RAG í•¨ìˆ˜ ---
def search_rag_file(rag_file_path, query):
    # íŒŒì¼ ê²½ë¡œì— ~ ìˆìœ¼ë©´ í™ˆ ë””ë ‰í† ë¦¬ë¡œ ë³€í™˜
    rag_file_path = os.path.expanduser(rag_file_path)
    if not rag_file_path or not os.path.isfile(rag_file_path):
        return ""
    with open(rag_file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    # ìœ ì‚¬ë„ ê²€ìƒ‰ ê°„ë‹¨ ë²„ì „: queryê°€ ë“¤ì–´ê°„ ì¤„ 3ê°œë§Œ ì¶”ì¶œ
    results = [line.strip() for line in lines if query.lower() in line.lower()]
    return "\n".join(results[:3]) if results else ""

# --- ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤ ---
class ModelManager:
    def __init__(self, num_models):
        self.models = []
        self.model_locks = []  # ê° ëª¨ë¸ë³„ ë½
        self.counter = 0
        self.counter_lock = threading.Lock()  # ì¹´ìš´í„°ìš© ë½
        self.load_models(num_models)
        
    def load_models(self, num_models):
        for i in range(num_models):
            print(f"Loading model instance {i+1}/{num_models}...")
            model = Llama(
                model_path=MODEL_PATH,
                n_threads=THREADS,
                n_ctx=CTX_SIZE,
                n_gpu_layers=GPU_LAYERS,
                low_vram=LOW_VRAM,
                verbose=False,
            )
            self.models.append(model)
            self.model_locks.append(threading.Lock())
            
            # ì²« ë²ˆì§¸ ëª¨ë¸ì— ëŒ€í•´ì„œë§Œ ì›Œë°ì—… ì‹¤í–‰
            if i == 0:
                try:
                    print("Warming up model...")
                    _ = model("ì›Œë°ì—…", max_tokens=1)
                    print("Model warmed up successfully")
                except Exception as e:
                    print(f"Warning: Warmup failed: {e}")
        
        print(f"All {num_models} model instances loaded successfully")
    
    def get_model(self):
        # ì›ìì ìœ¼ë¡œ ì¹´ìš´í„° ì¦ê°€ ë° ëª¨ë¸ ì„ íƒ
        with self.counter_lock:
            model_idx = self.counter % len(self.models)
            self.counter += 1
            print(f"Selected model instance: {model_idx}")
            return model_idx
    
    def execute(self, model_idx, prompt, max_tokens):
        # ì„ íƒëœ ëª¨ë¸ì— ëŒ€í•œ ë½ íšë“
        with self.model_locks[model_idx]:
            print(f"Executing on model instance {model_idx}")
            return self.models[model_idx](prompt, max_tokens=max_tokens)

# ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model_manager = None

# ê° ìš”ì²­ì— ëŒ€í•œ ì„¸ë§ˆí¬ì–´ (ë™ì‹œ ìš”ì²­ ì œí•œ)
INFERENCE_SEM = threading.Semaphore(MAX_CONCURRENT)

@app.on_event("startup")
def on_startup():
    global model_manager
    print("Initializing model manager...")
    model_manager = ModelManager(NUM_MODELS)
    print("Server startup complete")

@app.get("/healthz")
def healthz():
    return {"status": "ok", "model": MODEL_PATH, "instances": len(model_manager.models) if model_manager else 0}

@app.post(
    "/v1/chat/completions",
    response_class=JSONResponse,
    dependencies=[Depends(get_api_key)]
)
async def chat(request: ChatRequest):
    full_prompt = []
    if request.system_prompt:
        full_prompt.append({"role": "system", "content": request.system_prompt})
    full_prompt += request.messages
    if request.user_prompt_input:
        full_prompt.append({"role": "user", "content": request.user_prompt_input})

    # RAG íŒŒì¼ì—ì„œ ë‚´ìš© ê²€ìƒ‰ (ì˜µì…˜)
    rag_snippet = ""
    if request.rag_file_path and request.rag_query:
        rag_snippet = search_rag_file(request.rag_file_path, request.rag_query)
        if rag_snippet:
            # í”„ë¡¬í”„íŠ¸ì— RAG ê²°ê³¼ ì¶”ê°€
            full_prompt.append({"role": "system", "content": f"[RAG_RESULT]\n{rag_snippet}"})

    prompt_text = "<bos>"  # ì‹œì‘ í† í°
    system_text = None

    for m in full_prompt:
        role = m["role"] if isinstance(m, dict) else m.role
        content = m["content"] if isinstance(m, dict) else m.content

        if role == "system":
            # GemmaëŠ” system ë©”ì‹œì§€ë¥¼ user block ì•ˆì— í¬í•¨ì‹œí‚¤ëŠ” ê²Œ ì¼ë°˜ì 
            system_text = content.strip()
        elif role == "user":
            prompt_text += f"<start_of_turn>user\n"
            if system_text:
                prompt_text += system_text + "\n\n"  # systemì„ user ë©”ì‹œì§€ì— í¬í•¨
                system_text = None  # system ë©”ì‹œì§€ëŠ” ì²« ë²ˆì§¸ user ë©”ì‹œì§€ì—ë§Œ í¬í•¨
            prompt_text += content.strip() + "\n<end_of_turn>\n"
        elif role == "assistant":
            prompt_text += f"<start_of_turn>model\n{content.strip()}\n<end_of_turn>\n"

    # ëª¨ë¸ ì‘ë‹µìš© ìë¦¬ ë‚¨ê¸°ê¸°
    prompt_text += "<start_of_turn>model\n"

    # ğŸ”’í† í° ê¸¸ì´ ì´ˆê³¼ ê²€ì‚¬
    prompt_tokens = model_manager.models[0].tokenize(prompt_text.encode("utf-8"))
    total_requested = len(prompt_tokens) + request.max_tokens
    if total_requested > CTX_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"âŒ Token limit exceeded: {total_requested} > {CTX_SIZE} (prompt: {len(prompt_tokens)} + max: {request.max_tokens})"
        )

    request_id = uuid.uuid4().hex
    print(f"Request {request_id}: Waiting for semaphore")
    INFERENCE_SEM.acquire()
    try:
        print(f"Request {request_id}: Acquired semaphore")
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ
        model_idx = model_manager.get_model()
        print(f"Request {request_id}: Selected model {model_idx}")
        
        # ì„ íƒëœ ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰
        result = model_manager.execute(model_idx, prompt_text, request.max_tokens)
        print(f"Request {request_id}: Inference completed with model {model_idx}")

        # ê²°ê³¼ ì²˜ë¦¬
        if isinstance(result, dict):
            if "choices" in result and isinstance(result["choices"], list):
                text = result["choices"][0]["text"]
            elif "text" in result:
                text = result["text"]
            else:
                raise ValueError("Unexpected response format: missing 'choices' or 'text'")
        else:
            raise ValueError("Model returned non-dict result")

        response = {
            "id": f"chatcmpl-{request_id[:14]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": os.path.basename(MODEL_PATH),
            "model_instance": model_idx,  # ì–´ë–¤ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ì²˜ë¦¬í–ˆëŠ”ì§€ ì¶”ê°€
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": text.strip()
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": result.get("prompt_tokens", 0),
                "completion_tokens": result.get("completion_tokens", 0),
                "total_tokens": result.get("total_tokens", 0)
            }
        }

        # ë””ë²„ê¹… ë¡œê·¸ ì €ì¥
        with open("/tmp/llm_log.txt", "a") as f:
            f.write(f"\n===== NEW REQUEST {request_id} (MODEL {model_idx}) =====\n")
            f.write("PROMPT:\n" + prompt_text + "\n\n")
            f.write("RAW RESULT:\n" + json.dumps(result, indent=2, default=str) + "\n\n")
            f.write("FINAL RESPONSE:\n" + json.dumps(response, indent=2) + "\n")

        print(f"Request {request_id}: Completed successfully")
        return JSONResponse(content=response)

    except Exception as e:
        traceback_str = traceback.format_exc()
        print(f"Request {request_id}: Error during model call: {e}")
        print(traceback_str)
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        INFERENCE_SEM.release()
        print(f"Request {request_id}: Released semaphore")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app:app",  # app ëª¨ë“ˆì˜ app ê°ì²´ (íŒŒì¼ëª…ì´ app.pyì¸ ê²½ìš°)
        host="0.0.0.0",
        port=30000,
        log_level="info",
        reload=False
    )
